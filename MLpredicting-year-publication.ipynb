{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import json\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will import the Json train and test files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>title</th>\n",
       "      <th>editor</th>\n",
       "      <th>year</th>\n",
       "      <th>publisher</th>\n",
       "      <th>author</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65904</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Coreference in Knowledge Editing</td>\n",
       "      <td></td>\n",
       "      <td>1998</td>\n",
       "      <td></td>\n",
       "      <td>[van Deemter, Kees, Power, Richard]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65905</th>\n",
       "      <td>proceedings</td>\n",
       "      <td>COLING 1965</td>\n",
       "      <td></td>\n",
       "      <td>1965</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65906</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Precision-focused Textual Inference</td>\n",
       "      <td></td>\n",
       "      <td>2007</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Bobrow, Daniel, Crouch, Dick, King, Tracy Hol...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65907</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Generative Knowledge Graph Construction: A Review</td>\n",
       "      <td></td>\n",
       "      <td>2022</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Ye, Hongbin, Zhang, Ningyu, Chen, Hui, Chen, ...</td>\n",
       "      <td>Generative Knowledge Graph Construction (KGC) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65908</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Mind the Gap: Data Enrichment in Dependency Pa...</td>\n",
       "      <td></td>\n",
       "      <td>2018</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Droganova, Kira, Ginter, Filip, Kanerva, Jenn...</td>\n",
       "      <td>In this paper, we focus on parsing rare and no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65909</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Optimizing the weighted sequence alignment alg...</td>\n",
       "      <td></td>\n",
       "      <td>2022</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Janicki, Maciej]</td>\n",
       "      <td>We present an optimized implementation of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65910</th>\n",
       "      <td>proceedings</td>\n",
       "      <td>Proceedings of the 25th Conference on Computat...</td>\n",
       "      <td>[Bisazza, Arianna, Abend, Omri]</td>\n",
       "      <td>2021</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65911</th>\n",
       "      <td>article</td>\n",
       "      <td>A Large-Scale Pseudoword-Based Evaluation Fram...</td>\n",
       "      <td></td>\n",
       "      <td>2014</td>\n",
       "      <td>MIT Press</td>\n",
       "      <td>[Pilehvar, Mohammad Taher, Navigli, Roberto]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65912</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>CIST System for CL-SciSumm 2016 Shared Task</td>\n",
       "      <td></td>\n",
       "      <td>2016</td>\n",
       "      <td></td>\n",
       "      <td>[Li, Lei, Mao, Liyuan, Zhang, Yazhao, Chi, Jun...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65913</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Ontology Engineering and Knowledge Extraction ...</td>\n",
       "      <td></td>\n",
       "      <td>2009</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Trapman, Jantine, Monachesi, Paola]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ENTRYTYPE                                              title  \\\n",
       "65904  inproceedings                   Coreference in Knowledge Editing   \n",
       "65905    proceedings                                        COLING 1965   \n",
       "65906  inproceedings                Precision-focused Textual Inference   \n",
       "65907  inproceedings  Generative Knowledge Graph Construction: A Review   \n",
       "65908  inproceedings  Mind the Gap: Data Enrichment in Dependency Pa...   \n",
       "65909  inproceedings  Optimizing the weighted sequence alignment alg...   \n",
       "65910    proceedings  Proceedings of the 25th Conference on Computat...   \n",
       "65911        article  A Large-Scale Pseudoword-Based Evaluation Fram...   \n",
       "65912  inproceedings        CIST System for CL-SciSumm 2016 Shared Task   \n",
       "65913  inproceedings  Ontology Engineering and Knowledge Extraction ...   \n",
       "\n",
       "                                editor  year  \\\n",
       "65904                                   1998   \n",
       "65905                                   1965   \n",
       "65906                                   2007   \n",
       "65907                                   2022   \n",
       "65908                                   2018   \n",
       "65909                                   2022   \n",
       "65910  [Bisazza, Arianna, Abend, Omri]  2021   \n",
       "65911                                   2014   \n",
       "65912                                   2016   \n",
       "65913                                   2009   \n",
       "\n",
       "                                       publisher  \\\n",
       "65904                                              \n",
       "65905                                              \n",
       "65906  Association for Computational Linguistics   \n",
       "65907  Association for Computational Linguistics   \n",
       "65908  Association for Computational Linguistics   \n",
       "65909  Association for Computational Linguistics   \n",
       "65910  Association for Computational Linguistics   \n",
       "65911                                  MIT Press   \n",
       "65912                                              \n",
       "65913  Association for Computational Linguistics   \n",
       "\n",
       "                                                  author  \\\n",
       "65904                [van Deemter, Kees, Power, Richard]   \n",
       "65905                                                      \n",
       "65906  [Bobrow, Daniel, Crouch, Dick, King, Tracy Hol...   \n",
       "65907  [Ye, Hongbin, Zhang, Ningyu, Chen, Hui, Chen, ...   \n",
       "65908  [Droganova, Kira, Ginter, Filip, Kanerva, Jenn...   \n",
       "65909                                  [Janicki, Maciej]   \n",
       "65910                                                      \n",
       "65911       [Pilehvar, Mohammad Taher, Navigli, Roberto]   \n",
       "65912  [Li, Lei, Mao, Liyuan, Zhang, Yazhao, Chi, Jun...   \n",
       "65913               [Trapman, Jantine, Monachesi, Paola]   \n",
       "\n",
       "                                                abstract  \n",
       "65904                                                     \n",
       "65905                                                     \n",
       "65906                                                     \n",
       "65907  Generative Knowledge Graph Construction (KGC) ...  \n",
       "65908  In this paper, we focus on parsing rare and no...  \n",
       "65909  We present an optimized implementation of the ...  \n",
       "65910                                                     \n",
       "65911                                                     \n",
       "65912                                                     \n",
       "65913                                                     "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opening JSON file\n",
    "f = \"/Users/Alejandro/Desktop/MASTER/Machine learning 23/train.json\"\n",
    "\n",
    "with open(f, 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "# we convert it into a pd dataframe\n",
    "train = pd.DataFrame(json_data).fillna(\"\")\n",
    "train.tail(10)\n",
    "\n",
    "# We visualize some lines of the training data set in a pandas dataframe to have an insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "count    46139.000000\n",
      "mean      2012.572141\n",
      "std         10.164280\n",
      "min       1952.000000\n",
      "25%       2008.000000\n",
      "50%       2016.000000\n",
      "75%       2020.000000\n",
      "max       2023.000000\n",
      "Name: year, dtype: float64\n",
      "Mean: 2012.5721407052602\n",
      "Standard Deviation: 10.164280477955465\n",
      "Standard Error: 0.04731976202929839\n",
      "Minimum: 1952\n",
      "Maximum: 2023\n",
      "25th Percentile (Q1): 2008.0\n",
      "Median (50th Percentile): 2016.0\n",
      "75th Percentile (Q3): 2020.0\n"
     ]
    }
   ],
   "source": [
    "# We calulate summary statistics to know more about our predicted variable\n",
    "\n",
    "train['year'] = pd.to_numeric(train['year'], errors='coerce')\n",
    "print(train['year'].dtype)\n",
    "year = train['year']\n",
    "\n",
    "summary_stats = year.describe()\n",
    "print(summary_stats)\n",
    "\n",
    "mean_value = year.mean()\n",
    "\n",
    "std_dev = year.std()\n",
    "\n",
    "se = std_dev / np.sqrt(len(year))\n",
    "\n",
    "max_value = year.max()\n",
    "min_value = year.min()\n",
    "\n",
    "q1 = year.quantile(0.25)\n",
    "median_value = year.median()\n",
    "q3 = year.quantile(0.75)\n",
    "\n",
    "# Display the results\n",
    "print(\"Mean:\", mean_value)\n",
    "print(\"Standard Deviation:\", std_dev)\n",
    "print(\"Standard Error:\", se)\n",
    "print(\"Minimum:\", min_value)\n",
    "print(\"Maximum:\", max_value)\n",
    "print(\"25th Percentile (Q1):\", q1)\n",
    "print(\"Median (50th Percentile):\", median_value)\n",
    "print(\"75th Percentile (Q3):\", q3)\n",
    "\n",
    "# This was to check if the algo was working compare to the std."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ENTRYTYPE', 'title', 'editor', 'year', 'publisher', 'author',\n",
      "       'abstract'],\n",
      "      dtype='object')\n",
      "Number of empty string cells in 'abstract': 23528\n"
     ]
    }
   ],
   "source": [
    "print(train.columns) # We check the name of the columns in the environment and then check because there seem to be many missing information\n",
    "\n",
    "column_name = 'abstract'  \n",
    "if column_name in train.columns:\n",
    "    empty_string_count = (train[column_name] == '').sum()\n",
    "    print(f\"Number of empty string cells in '{column_name}': {empty_string_count}\")\n",
    "else:\n",
    "    print(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46139, 5)\n",
      "\n",
      "Percentage of column 'abstract' missing values: 51.04 %\n"
     ]
    }
   ],
   "source": [
    "# Now we check the dimensions of the pandas data frame because it looks like a problem of missing data in some columns.\n",
    "\n",
    "print(train.shape) \n",
    "print()\n",
    "print(\"Percentage of column 'abstract' missing values:\", round((33642*100)/65914, 2),\"%\")\n",
    "\n",
    "# There is more than 50% of missing data for the abstract column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'editor' not found in the DataFrame.\n",
      "Percentage of column 'editor' missing values: 97.76 %\n",
      "\n",
      "Number of empty string cells in 'publisher': 5686\n",
      "Percentage of column 'publisher' missing values: 12.44 %\n",
      "\n",
      "Number of empty string cells in 'author': 1728\n",
      "Percentage of column 'author' missing values: 3.64 %\n",
      "\n",
      "Number of empty string cells in 'year': 0\n",
      "Percentage of column 'author' missing values: 0.0 %\n"
     ]
    }
   ],
   "source": [
    "#Now we do the same for the rest of the columns\n",
    "\n",
    "column_name = 'editor'  # Replace with your column name\n",
    "if column_name in train.columns:\n",
    "    empty_string_count = (train[column_name] == '').sum()\n",
    "    print(f\"Number of empty string cells in '{column_name}': {empty_string_count}\")\n",
    "else:\n",
    "    print(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "print(\"Percentage of column 'editor' missing values:\", round((64438*100)/65914, 2),\"%\")\n",
    "\n",
    "\n",
    "print()\n",
    "column_name = 'publisher'  # Replace with your column name\n",
    "if column_name in train.columns:\n",
    "    empty_string_count = (train[column_name] == '').sum()\n",
    "    print(f\"Number of empty string cells in '{column_name}': {empty_string_count}\")\n",
    "else:\n",
    "    print(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "print(\"Percentage of column 'publisher' missing values:\", round((8201*100)/65914, 2),\"%\")\n",
    "\n",
    "\n",
    "print()\n",
    "column_name = 'author'  # Replace with your column name\n",
    "if column_name in train.columns:\n",
    "    empty_string_count = (train[column_name] == '').sum()\n",
    "    print(f\"Number of empty string cells in '{column_name}': {empty_string_count}\")\n",
    "else:\n",
    "    print(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "print(\"Percentage of column 'author' missing values:\", round((2399*100)/65914, 2),\"%\")\n",
    "\n",
    "print()\n",
    "column_name = 'year'  # Replace with your column name\n",
    "if column_name in train.columns:\n",
    "    empty_string_count = (train[column_name] == '').sum()\n",
    "    print(f\"Number of empty string cells in '{column_name}': {empty_string_count}\")\n",
    "else:\n",
    "    print(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "print(\"Percentage of column 'author' missing values:\", round((0*100)/65914, 2),\"%\")\n",
    "\n",
    "# Check how round function was used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['editor'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-1669edaf1797>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcolumn_to_remove\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'editor'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn_to_remove\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alejandro\\OneDrive\\Attachments\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4306\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4307\u001b[0m         \"\"\"\n\u001b[1;32m-> 4308\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   4309\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4310\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alejandro\\OneDrive\\Attachments\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4151\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4152\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4153\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4155\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alejandro\\OneDrive\\Attachments\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   4186\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4187\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4188\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4189\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alejandro\\OneDrive\\Attachments\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   5589\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5590\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5591\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5592\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5593\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['editor'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# There so many unavailable data for the 'editor' column, that we delete it from our pd data frame.\n",
    "\n",
    "column_to_remove = 'editor'\n",
    "train = train.drop(column_to_remove, axis=1)\n",
    "\n",
    "pd.DataFrame(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['abstract'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-721477869e7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcolumn_to_remove2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'abstract'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn_to_remove2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alejandro\\OneDrive\\Attachments\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4306\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4307\u001b[0m         \"\"\"\n\u001b[1;32m-> 4308\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   4309\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4310\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alejandro\\OneDrive\\Attachments\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4151\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4152\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4153\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4155\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alejandro\\OneDrive\\Attachments\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   4186\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4187\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4188\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4189\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alejandro\\OneDrive\\Attachments\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   5589\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5590\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5591\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5592\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5593\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['abstract'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Lets do the same with abstract to try to improve our MAE.\n",
    "\n",
    "column_to_remove2 = 'abstract'\n",
    "train = train.drop(column_to_remove2, axis=1)\n",
    "\n",
    "pd.DataFrame(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = \"/Users/Alejandro/Desktop/MASTER/Machine learning 23/test.json\"\n",
    "\n",
    "with open(f, 'r') as file:\n",
    "    json_data2 = json.load(file)\n",
    "\n",
    "# we convert it into a pd dataframe\n",
    "test = pd.DataFrame(json_data2).fillna(\"\")\n",
    "\n",
    "# We some lines of the testing data set to have an insight, and we see there is the predictor column missing so check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ENTRYTYPE                                              title  \\\n",
      "0      inproceedings          Learning to lemmatise Polish noun phrases   \n",
      "1      inproceedings  The Treebanked Conspiracy. Actors and Actions ...   \n",
      "2      inproceedings       Linguistic structure and machine translation   \n",
      "3      inproceedings  NSEmo at EmoInt-2017: An Ensemble to Predict E...   \n",
      "4      inproceedings     Explaining data using causal Bayesian networks   \n",
      "...              ...                                                ...   \n",
      "21967  inproceedings  Scalable Font Reconstruction with Dual Latent ...   \n",
      "21968  inproceedings  UniGeo: Unifying Geometry Logical Reasoning vi...   \n",
      "21969  inproceedings  Gradient-guided Unsupervised Lexically Constra...   \n",
      "21970  inproceedings  Semantically Constrained Multilayer Annotation...   \n",
      "21971  inproceedings  Annotating Students' Understanding of Science ...   \n",
      "\n",
      "      editor                                       publisher  \\\n",
      "0                  Association for Computational Linguistics   \n",
      "1                                                              \n",
      "2                                                              \n",
      "3                  Association for Computational Linguistics   \n",
      "4                  Association for Computational Linguistics   \n",
      "...      ...                                             ...   \n",
      "21967              Association for Computational Linguistics   \n",
      "21968              Association for Computational Linguistics   \n",
      "21969              Association for Computational Linguistics   \n",
      "21970              Association for Computational Linguistics   \n",
      "21971         European Language Resources Association (ELRA)   \n",
      "\n",
      "                                                  author  \\\n",
      "0                                 ['Radziszewski, Adam']   \n",
      "1      ['Passarotti, Marco', 'González Saavedra, Berta']   \n",
      "2                                    ['Lamb, Sydney M.']   \n",
      "3      ['Madisetty, Sreekanth', 'Desarkar, Maunendra ...   \n",
      "4                                     ['Sevilla, Jaime']   \n",
      "...                                                  ...   \n",
      "21967  ['Srivatsan, Nikita', 'Wu, Si', 'Barron, Jonat...   \n",
      "21968  ['Chen, Jiaqi', 'Li, Tong', 'Qin, Jinghui', 'L...   \n",
      "21969                                       ['Sha, Lei']   \n",
      "21970  ['Prange, Jakob', 'Schneider, Nathan', 'Abend,...   \n",
      "21971  ['Nielsen, Rodney D.', 'Ward, Wayne', 'Martin,...   \n",
      "\n",
      "                                                abstract  \n",
      "0                                                         \n",
      "1                                                         \n",
      "2      If one understands the nature of linguistic st...  \n",
      "3      In this paper, we describe a method to predict...  \n",
      "4      I introduce Causal Bayesian Networks as a form...  \n",
      "...                                                  ...  \n",
      "21967  We propose a deep generative model that perfor...  \n",
      "21968  Geometry problem solving is a well-recognized ...  \n",
      "21969  Lexically constrained generation requires the ...  \n",
      "21970  We propose a coreference annotation scheme as ...  \n",
      "21971  This paper summarizes the annotation of fine-g...  \n",
      "\n",
      "[21972 rows x 6 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>title</th>\n",
       "      <th>publisher</th>\n",
       "      <th>author</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Learning to lemmatise Polish noun phrases</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>['Radziszewski, Adam']</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>The Treebanked Conspiracy. Actors and Actions ...</td>\n",
       "      <td></td>\n",
       "      <td>['Passarotti, Marco', 'González Saavedra, Berta']</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Linguistic structure and machine translation</td>\n",
       "      <td></td>\n",
       "      <td>['Lamb, Sydney M.']</td>\n",
       "      <td>If one understands the nature of linguistic st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>NSEmo at EmoInt-2017: An Ensemble to Predict E...</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>['Madisetty, Sreekanth', 'Desarkar, Maunendra ...</td>\n",
       "      <td>In this paper, we describe a method to predict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Explaining data using causal Bayesian networks</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>['Sevilla, Jaime']</td>\n",
       "      <td>I introduce Causal Bayesian Networks as a form...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21967</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Scalable Font Reconstruction with Dual Latent ...</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>['Srivatsan, Nikita', 'Wu, Si', 'Barron, Jonat...</td>\n",
       "      <td>We propose a deep generative model that perfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21968</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>UniGeo: Unifying Geometry Logical Reasoning vi...</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>['Chen, Jiaqi', 'Li, Tong', 'Qin, Jinghui', 'L...</td>\n",
       "      <td>Geometry problem solving is a well-recognized ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21969</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Gradient-guided Unsupervised Lexically Constra...</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>['Sha, Lei']</td>\n",
       "      <td>Lexically constrained generation requires the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21970</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Semantically Constrained Multilayer Annotation...</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>['Prange, Jakob', 'Schneider, Nathan', 'Abend,...</td>\n",
       "      <td>We propose a coreference annotation scheme as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21971</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Annotating Students' Understanding of Science ...</td>\n",
       "      <td>European Language Resources Association (ELRA)</td>\n",
       "      <td>['Nielsen, Rodney D.', 'Ward, Wayne', 'Martin,...</td>\n",
       "      <td>This paper summarizes the annotation of fine-g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21972 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ENTRYTYPE                                              title  \\\n",
       "0      inproceedings          Learning to lemmatise Polish noun phrases   \n",
       "1      inproceedings  The Treebanked Conspiracy. Actors and Actions ...   \n",
       "2      inproceedings       Linguistic structure and machine translation   \n",
       "3      inproceedings  NSEmo at EmoInt-2017: An Ensemble to Predict E...   \n",
       "4      inproceedings     Explaining data using causal Bayesian networks   \n",
       "...              ...                                                ...   \n",
       "21967  inproceedings  Scalable Font Reconstruction with Dual Latent ...   \n",
       "21968  inproceedings  UniGeo: Unifying Geometry Logical Reasoning vi...   \n",
       "21969  inproceedings  Gradient-guided Unsupervised Lexically Constra...   \n",
       "21970  inproceedings  Semantically Constrained Multilayer Annotation...   \n",
       "21971  inproceedings  Annotating Students' Understanding of Science ...   \n",
       "\n",
       "                                            publisher  \\\n",
       "0           Association for Computational Linguistics   \n",
       "1                                                       \n",
       "2                                                       \n",
       "3           Association for Computational Linguistics   \n",
       "4           Association for Computational Linguistics   \n",
       "...                                               ...   \n",
       "21967       Association for Computational Linguistics   \n",
       "21968       Association for Computational Linguistics   \n",
       "21969       Association for Computational Linguistics   \n",
       "21970       Association for Computational Linguistics   \n",
       "21971  European Language Resources Association (ELRA)   \n",
       "\n",
       "                                                  author  \\\n",
       "0                                 ['Radziszewski, Adam']   \n",
       "1      ['Passarotti, Marco', 'González Saavedra, Berta']   \n",
       "2                                    ['Lamb, Sydney M.']   \n",
       "3      ['Madisetty, Sreekanth', 'Desarkar, Maunendra ...   \n",
       "4                                     ['Sevilla, Jaime']   \n",
       "...                                                  ...   \n",
       "21967  ['Srivatsan, Nikita', 'Wu, Si', 'Barron, Jonat...   \n",
       "21968  ['Chen, Jiaqi', 'Li, Tong', 'Qin, Jinghui', 'L...   \n",
       "21969                                       ['Sha, Lei']   \n",
       "21970  ['Prange, Jakob', 'Schneider, Nathan', 'Abend,...   \n",
       "21971  ['Nielsen, Rodney D.', 'Ward, Wayne', 'Martin,...   \n",
       "\n",
       "                                                abstract  \n",
       "0                                                         \n",
       "1                                                         \n",
       "2      If one understands the nature of linguistic st...  \n",
       "3      In this paper, we describe a method to predict...  \n",
       "4      I introduce Causal Bayesian Networks as a form...  \n",
       "...                                                  ...  \n",
       "21967  We propose a deep generative model that perfor...  \n",
       "21968  Geometry problem solving is a well-recognized ...  \n",
       "21969  Lexically constrained generation requires the ...  \n",
       "21970  We propose a coreference annotation scheme as ...  \n",
       "21971  This paper summarizes the annotation of fine-g...  \n",
       "\n",
       "[21972 rows x 5 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets delete the column 'editor' for the testing set as well.\n",
    "# Delete editor column\n",
    "\n",
    "print(test)\n",
    "\n",
    "column_to_remove = 'editor'\n",
    "test = test.drop(column_to_remove, axis=1)\n",
    "\n",
    "pd.DataFrame(test)  \n",
    "\n",
    "# You forgot to delete the abstract column for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ENTRYTYPE                                              title  \\\n",
      "0      inproceedings          Learning to lemmatise Polish noun phrases   \n",
      "1      inproceedings  The Treebanked Conspiracy. Actors and Actions ...   \n",
      "2      inproceedings       Linguistic structure and machine translation   \n",
      "3      inproceedings  NSEmo at EmoInt-2017: An Ensemble to Predict E...   \n",
      "4      inproceedings     Explaining data using causal Bayesian networks   \n",
      "...              ...                                                ...   \n",
      "21967  inproceedings  Scalable Font Reconstruction with Dual Latent ...   \n",
      "21968  inproceedings  UniGeo: Unifying Geometry Logical Reasoning vi...   \n",
      "21969  inproceedings  Gradient-guided Unsupervised Lexically Constra...   \n",
      "21970  inproceedings  Semantically Constrained Multilayer Annotation...   \n",
      "21971  inproceedings  Annotating Students' Understanding of Science ...   \n",
      "\n",
      "      editor                                       publisher  \\\n",
      "0                  Association for Computational Linguistics   \n",
      "1                                                              \n",
      "2                                                              \n",
      "3                  Association for Computational Linguistics   \n",
      "4                  Association for Computational Linguistics   \n",
      "...      ...                                             ...   \n",
      "21967              Association for Computational Linguistics   \n",
      "21968              Association for Computational Linguistics   \n",
      "21969              Association for Computational Linguistics   \n",
      "21970              Association for Computational Linguistics   \n",
      "21971         European Language Resources Association (ELRA)   \n",
      "\n",
      "                                                  author  \\\n",
      "0                                   [Radziszewski, Adam]   \n",
      "1          [Passarotti, Marco, González Saavedra, Berta]   \n",
      "2                                      [Lamb, Sydney M.]   \n",
      "3      [Madisetty, Sreekanth, Desarkar, Maunendra San...   \n",
      "4                                       [Sevilla, Jaime]   \n",
      "...                                                  ...   \n",
      "21967  [Srivatsan, Nikita, Wu, Si, Barron, Jonathan, ...   \n",
      "21968  [Chen, Jiaqi, Li, Tong, Qin, Jinghui, Lu, Pan,...   \n",
      "21969                                         [Sha, Lei]   \n",
      "21970    [Prange, Jakob, Schneider, Nathan, Abend, Omri]   \n",
      "21971  [Nielsen, Rodney D., Ward, Wayne, Martin, Jame...   \n",
      "\n",
      "                                                abstract  \n",
      "0                                                         \n",
      "1                                                         \n",
      "2      If one understands the nature of linguistic st...  \n",
      "3      In this paper, we describe a method to predict...  \n",
      "4      I introduce Causal Bayesian Networks as a form...  \n",
      "...                                                  ...  \n",
      "21967  We propose a deep generative model that perfor...  \n",
      "21968  Geometry problem solving is a well-recognized ...  \n",
      "21969  Lexically constrained generation requires the ...  \n",
      "21970  We propose a coreference annotation scheme as ...  \n",
      "21971  This paper summarizes the annotation of fine-g...  \n",
      "\n",
      "[21972 rows x 6 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>title</th>\n",
       "      <th>editor</th>\n",
       "      <th>publisher</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Learning to lemmatise Polish noun phrases</td>\n",
       "      <td></td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Radziszewski, Adam]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>The Treebanked Conspiracy. Actors and Actions ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Passarotti, Marco, González Saavedra, Berta]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Linguistic structure and machine translation</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Lamb, Sydney M.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>NSEmo at EmoInt-2017: An Ensemble to Predict E...</td>\n",
       "      <td></td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Madisetty, Sreekanth, Desarkar, Maunendra San...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Explaining data using causal Bayesian networks</td>\n",
       "      <td></td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Sevilla, Jaime]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21967</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Scalable Font Reconstruction with Dual Latent ...</td>\n",
       "      <td></td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Srivatsan, Nikita, Wu, Si, Barron, Jonathan, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21968</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>UniGeo: Unifying Geometry Logical Reasoning vi...</td>\n",
       "      <td></td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Chen, Jiaqi, Li, Tong, Qin, Jinghui, Lu, Pan,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21969</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Gradient-guided Unsupervised Lexically Constra...</td>\n",
       "      <td></td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Sha, Lei]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21970</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Semantically Constrained Multilayer Annotation...</td>\n",
       "      <td></td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Prange, Jakob, Schneider, Nathan, Abend, Omri]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21971</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Annotating Students' Understanding of Science ...</td>\n",
       "      <td></td>\n",
       "      <td>European Language Resources Association (ELRA)</td>\n",
       "      <td>[Nielsen, Rodney D., Ward, Wayne, Martin, Jame...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21972 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ENTRYTYPE                                              title  \\\n",
       "0      inproceedings          Learning to lemmatise Polish noun phrases   \n",
       "1      inproceedings  The Treebanked Conspiracy. Actors and Actions ...   \n",
       "2      inproceedings       Linguistic structure and machine translation   \n",
       "3      inproceedings  NSEmo at EmoInt-2017: An Ensemble to Predict E...   \n",
       "4      inproceedings     Explaining data using causal Bayesian networks   \n",
       "...              ...                                                ...   \n",
       "21967  inproceedings  Scalable Font Reconstruction with Dual Latent ...   \n",
       "21968  inproceedings  UniGeo: Unifying Geometry Logical Reasoning vi...   \n",
       "21969  inproceedings  Gradient-guided Unsupervised Lexically Constra...   \n",
       "21970  inproceedings  Semantically Constrained Multilayer Annotation...   \n",
       "21971  inproceedings  Annotating Students' Understanding of Science ...   \n",
       "\n",
       "      editor                                       publisher  \\\n",
       "0                  Association for Computational Linguistics   \n",
       "1                                                              \n",
       "2                                                              \n",
       "3                  Association for Computational Linguistics   \n",
       "4                  Association for Computational Linguistics   \n",
       "...      ...                                             ...   \n",
       "21967              Association for Computational Linguistics   \n",
       "21968              Association for Computational Linguistics   \n",
       "21969              Association for Computational Linguistics   \n",
       "21970              Association for Computational Linguistics   \n",
       "21971         European Language Resources Association (ELRA)   \n",
       "\n",
       "                                                  author  \n",
       "0                                   [Radziszewski, Adam]  \n",
       "1          [Passarotti, Marco, González Saavedra, Berta]  \n",
       "2                                      [Lamb, Sydney M.]  \n",
       "3      [Madisetty, Sreekanth, Desarkar, Maunendra San...  \n",
       "4                                       [Sevilla, Jaime]  \n",
       "...                                                  ...  \n",
       "21967  [Srivatsan, Nikita, Wu, Si, Barron, Jonathan, ...  \n",
       "21968  [Chen, Jiaqi, Li, Tong, Qin, Jinghui, Lu, Pan,...  \n",
       "21969                                         [Sha, Lei]  \n",
       "21970    [Prange, Jakob, Schneider, Nathan, Abend, Omri]  \n",
       "21971  [Nielsen, Rodney D., Ward, Wayne, Martin, Jame...  \n",
       "\n",
       "[21972 rows x 5 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we delete the 'abstract' column in the test set.\n",
    "\n",
    "print(test)\n",
    "\n",
    "column_to_remove = 'abstract'\n",
    "test = test.drop(column_to_remove, axis=1)\n",
    "\n",
    "pd.DataFrame(test)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>title</th>\n",
       "      <th>editor</th>\n",
       "      <th>year</th>\n",
       "      <th>publisher</th>\n",
       "      <th>author</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12680</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Question-Answering Based on Virtually Integrat...</td>\n",
       "      <td></td>\n",
       "      <td>2003</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>['Choi, Key-Sun', 'Kim, Jae-Ho', 'Miyazaki, Ma...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17292</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>AMI&amp;ERIC: How to Learn with Naive Bayes and Pr...</td>\n",
       "      <td></td>\n",
       "      <td>2013</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>['Dermouche, Mohamed', 'Khouas, Leila', 'Velci...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33265</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Inducing Gazetteers for Named Entity Recogniti...</td>\n",
       "      <td></td>\n",
       "      <td>2008</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[\"Kazama, Jun'ichi\", 'Torisawa, Kentaro']</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52850</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Leveraging Explicit Lexico-logical Alignments ...</td>\n",
       "      <td></td>\n",
       "      <td>2022</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>['Sun, Runxin', 'He, Shizhu', 'Zhu, Chong', 'H...</td>\n",
       "      <td>Text-to-SQL aims to parse natural language que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>CLAM: Quickly deploy NLP command-line tools on...</td>\n",
       "      <td></td>\n",
       "      <td>2014</td>\n",
       "      <td>Dublin City University and Association for Com...</td>\n",
       "      <td>['van Gompel, Maarten', 'Reynaert, Martin']</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6236</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Deep Span Representations for Named Entity Rec...</td>\n",
       "      <td></td>\n",
       "      <td>2023</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>['Zhu, Enwei', 'Liu, Yiyang', 'Li, Jinpeng']</td>\n",
       "      <td>Span-based models are one of the most straight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63616</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Comparing a Hand-crafted to an Automatically G...</td>\n",
       "      <td></td>\n",
       "      <td>2019</td>\n",
       "      <td>Incoma Ltd., Shoumen, Bulgaria</td>\n",
       "      <td>['Mouratidis, Despoina', 'Kermanidis, Katia Li...</td>\n",
       "      <td>The automatic evaluation of machine translatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6396</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Thai Stock News Sentiment Classification using...</td>\n",
       "      <td></td>\n",
       "      <td>2015</td>\n",
       "      <td></td>\n",
       "      <td>['Netisopakul, Ponrudee', 'Chattupan, Apinan']</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28216</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Effects of Empty Categories on Machine Transla...</td>\n",
       "      <td></td>\n",
       "      <td>2010</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>['Chung, Tagyoung', 'Gildea, Daniel']</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62486</th>\n",
       "      <td>proceedings</td>\n",
       "      <td>Proceedings of the 18th Conference on Natural ...</td>\n",
       "      <td>[Schaefer, Robin, Bai, Xiaoyu, Stede, Manfred,...</td>\n",
       "      <td>2022</td>\n",
       "      <td>KONVENS 2022 Organizers</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19775 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ENTRYTYPE                                              title  \\\n",
       "12680  inproceedings  Question-Answering Based on Virtually Integrat...   \n",
       "17292  inproceedings  AMI&ERIC: How to Learn with Naive Bayes and Pr...   \n",
       "33265  inproceedings  Inducing Gazetteers for Named Entity Recogniti...   \n",
       "52850  inproceedings  Leveraging Explicit Lexico-logical Alignments ...   \n",
       "2298   inproceedings  CLAM: Quickly deploy NLP command-line tools on...   \n",
       "...              ...                                                ...   \n",
       "6236   inproceedings  Deep Span Representations for Named Entity Rec...   \n",
       "63616  inproceedings  Comparing a Hand-crafted to an Automatically G...   \n",
       "6396   inproceedings  Thai Stock News Sentiment Classification using...   \n",
       "28216  inproceedings  Effects of Empty Categories on Machine Transla...   \n",
       "62486    proceedings  Proceedings of the 18th Conference on Natural ...   \n",
       "\n",
       "                                                  editor  year  \\\n",
       "12680                                                     2003   \n",
       "17292                                                     2013   \n",
       "33265                                                     2008   \n",
       "52850                                                     2022   \n",
       "2298                                                      2014   \n",
       "...                                                  ...   ...   \n",
       "6236                                                      2023   \n",
       "63616                                                     2019   \n",
       "6396                                                      2015   \n",
       "28216                                                     2010   \n",
       "62486  [Schaefer, Robin, Bai, Xiaoyu, Stede, Manfred,...  2022   \n",
       "\n",
       "                                               publisher  \\\n",
       "12680          Association for Computational Linguistics   \n",
       "17292          Association for Computational Linguistics   \n",
       "33265          Association for Computational Linguistics   \n",
       "52850          Association for Computational Linguistics   \n",
       "2298   Dublin City University and Association for Com...   \n",
       "...                                                  ...   \n",
       "6236           Association for Computational Linguistics   \n",
       "63616                     Incoma Ltd., Shoumen, Bulgaria   \n",
       "6396                                                       \n",
       "28216          Association for Computational Linguistics   \n",
       "62486                            KONVENS 2022 Organizers   \n",
       "\n",
       "                                                  author  \\\n",
       "12680  ['Choi, Key-Sun', 'Kim, Jae-Ho', 'Miyazaki, Ma...   \n",
       "17292  ['Dermouche, Mohamed', 'Khouas, Leila', 'Velci...   \n",
       "33265          [\"Kazama, Jun'ichi\", 'Torisawa, Kentaro']   \n",
       "52850  ['Sun, Runxin', 'He, Shizhu', 'Zhu, Chong', 'H...   \n",
       "2298         ['van Gompel, Maarten', 'Reynaert, Martin']   \n",
       "...                                                  ...   \n",
       "6236        ['Zhu, Enwei', 'Liu, Yiyang', 'Li, Jinpeng']   \n",
       "63616  ['Mouratidis, Despoina', 'Kermanidis, Katia Li...   \n",
       "6396      ['Netisopakul, Ponrudee', 'Chattupan, Apinan']   \n",
       "28216              ['Chung, Tagyoung', 'Gildea, Daniel']   \n",
       "62486                                                      \n",
       "\n",
       "                                                abstract  \n",
       "12680                                                     \n",
       "17292                                                     \n",
       "33265                                                     \n",
       "52850  Text-to-SQL aims to parse natural language que...  \n",
       "2298                                                      \n",
       "...                                                  ...  \n",
       "6236   Span-based models are one of the most straight...  \n",
       "63616  The automatic evaluation of machine translatio...  \n",
       "6396                                                      \n",
       "28216                                                     \n",
       "62486                                                     \n",
       "\n",
       "[19775 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train, validation = train_test_split(train, train_size = 0.70, random_state=42)       # We create a validation set and see it on a pandas data frame\n",
    "pd.DataFrame(validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 6 features, but ColumnTransformer is expecting 4 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-f91d76e50598>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mdummy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mridge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mridge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alejandro\\OneDrive\\Attachments\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alejandro\\OneDrive\\Attachments\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 418\u001b[1;33m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    419\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alejandro\\OneDrive\\Attachments\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    555\u001b[0m             \u001b[0mX_feature_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m         if (self._feature_names_in is not None and\n\u001b[0;32m    559\u001b[0m             \u001b[0mX_feature_names\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alejandro\\OneDrive\\Attachments\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    366\u001b[0m                 \u001b[1;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m                 f\"is expecting {self.n_features_in_} features as input.\")\n",
      "\u001b[1;31mValueError\u001b[0m: X has 6 features, but ColumnTransformer is expecting 4 features as input."
     ]
    }
   ],
   "source": [
    "test['title'] = test['title'].astype(str)\n",
    "test['author'] = test['author'].astype(str)\n",
    "\n",
    "featurizer = ColumnTransformer(\n",
    "        transformers=[(\"title\", CountVectorizer(), \"title\"),\n",
    "                (\"author\", CountVectorizer(), \"author\"),\n",
    "                (\"ENTRYTYPE\", CountVectorizer(), \"ENTRYTYPE\"),\n",
    "                (\"publisher\", CountVectorizer(), \"publisher\")],\n",
    "        remainder='drop')\n",
    "\n",
    "dummy = make_pipeline(featurizer, DummyRegressor(strategy = 'mean'))\n",
    "ridge = make_pipeline(featurizer, Ridge())\n",
    "logging.info(\"Fitting models\")\n",
    "dummy.fit(train.drop('year', axis=1), train['year'].values)\n",
    "ridge.fit(train.drop('year', axis=1), train['year'].values)\n",
    "err = mean_absolute_error(validation['year'].values, ridge.predict(validation.drop('year', axis=1)))\n",
    "print(validation.dtypes)\n",
    "print(err)\n",
    "\n",
    "err = mean_absolute_error(test['year'].values, ridge.predict(test.drop('year', axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 6 features, but ColumnTransformer is expecting 4 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-716e0a7cbd47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mridge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mridge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alejandro\\OneDrive\\Attachments\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alejandro\\OneDrive\\Attachments\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 418\u001b[1;33m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    419\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alejandro\\OneDrive\\Attachments\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    555\u001b[0m             \u001b[0mX_feature_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m         if (self._feature_names_in is not None and\n\u001b[0;32m    559\u001b[0m             \u001b[0mX_feature_names\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Alejandro\\OneDrive\\Attachments\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    366\u001b[0m                 \u001b[1;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m                 f\"is expecting {self.n_features_in_} features as input.\")\n",
      "\u001b[1;31mValueError\u001b[0m: X has 6 features, but ColumnTransformer is expecting 4 features as input."
     ]
    }
   ],
   "source": [
    "#implement one hot encoding for entrytype and publisher although publisher has more unique entries to one hot encode\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import logging\n",
    "\n",
    "train['title'] = train['title'].astype(str)\n",
    "train['author'] = train['author'].astype(str)\n",
    "train['year'] = pd.to_numeric(train['year'], errors='coerce')\n",
    "\n",
    "validation['title'] = validation['title'].astype(str)\n",
    "validation['author'] = validation['author'].astype(str)\n",
    "validation['year'] = pd.to_numeric(validation['year'], errors='coerce')\n",
    "\n",
    "# Assuming 'ENTRYTYPE' and 'publisher' are categorical columns\n",
    "categorical_columns = ['ENTRYTYPE', 'publisher']\n",
    "\n",
    "# Create a column transformer with one-hot encoding for categorical variables\n",
    "featurizer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"title\", TfidfVectorizer(), \"title\"),\n",
    "        (\"author\", TfidfVectorizer(), \"author\"),\n",
    "        (\"categorical\", OneHotEncoder(), categorical_columns)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "dummy = make_pipeline(featurizer, DummyRegressor(strategy='mean'))\n",
    "ridge = make_pipeline(featurizer, Ridge())\n",
    "\n",
    "logging.info(\"Fitting models\")\n",
    "dummy.fit(train.drop('year', axis=1), train['year'].values)\n",
    "ridge.fit(train.drop('year', axis=1), train['year'].values)\n",
    "\n",
    "err = mean_absolute_error(validation['year'].values, ridge.predict(validation.drop('year', axis=1)))\n",
    "print(validation.dtypes)\n",
    "print(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-71dd750f5f27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m featurizer = ColumnTransformer(\n\u001b[0;32m      6\u001b[0m     transformers=[\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[1;34m\"title\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"title\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[1;34m\"author\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"author\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[1;34m\"ENTRYTYPE\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategorical_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming only 'ENTRYTYPE' is a categorical column\n",
    "categorical_columns = ['ENTRYTYPE']\n",
    "\n",
    "# Create a column transformer with one-hot encoding for 'ENTRYTYPE' only\n",
    "featurizer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"title\", TfidfVectorizer(), \"title\"),\n",
    "        (\"author\", TfidfVectorizer(), \"author\"),\n",
    "        (\"ENTRYTYPE\", OneHotEncoder(), categorical_columns),\n",
    "        (\"publisher\", TfidfVectorizer(), \"publisher\")],\n",
    "        remainder='drop')\n",
    "\n",
    "dummy = make_pipeline(featurizer, DummyRegressor(strategy='mean'))\n",
    "ridge = make_pipeline(featurizer, Ridge())\n",
    "\n",
    "logging.info(\"Fitting models\")\n",
    "dummy.fit(train.drop('year', axis=1), train['year'].values)\n",
    "ridge.fit(train.drop('year', axis=1), train['year'].values)\n",
    "\n",
    "err = mean_absolute_error(validation['year'].values, ridge.predict(validation.drop('year', axis=1)))\n",
    "print(validation.dtypes)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = train.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error on Test Set: 4.399318304314843\n",
      "Wall time: 22h 33min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Constants\n",
    "TITLE_COLUMN = 'title'\n",
    "TARGET_COLUMN = 'year'\n",
    "\n",
    "# Feature engineering\n",
    "categorical_columns = ['ENTRYTYPE']\n",
    "\n",
    "featurizer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"title\", TfidfVectorizer(), \"title\"),\n",
    "        (\"author\", TfidfVectorizer(), \"author\"),\n",
    "        (\"ENTRYTYPE\", OneHotEncoder(), categorical_columns),\n",
    "        (\"publisher\", TfidfVectorizer(), \"publisher\")],\n",
    "        remainder='drop')\n",
    "\n",
    "dummy = make_pipeline(featurizer, DummyRegressor(strategy='mean'))\n",
    "ridge = make_pipeline(featurizer, Ridge())\n",
    "\n",
    "logging.info(\"Fitting models\")\n",
    "dummy.fit(train.drop('year', axis=1), train['year'].values)\n",
    "ridge.fit(train.drop('year', axis=1), train['year'].values)\n",
    "\n",
    "# Create pipelines\n",
    "random_forest = make_pipeline(featurizer, RandomForestRegressor(n_estimators = 500, random_state=123, n_jobs = -1, max_depth = 50))\n",
    "\n",
    "# Assuming your data is split into features and target\n",
    "X = train_sample.drop(TARGET_COLUMN, axis=1)\n",
    "y = train_sample[TARGET_COLUMN]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = random_forest.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "err = mean_absolute_error(y_test, predictions)\n",
    "print(f'Mean Absolute Error on Test Set: {err}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time #4.86     5.09  5.10   \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Constants\n",
    "TITLE_COLUMN = 'title'\n",
    "TARGET_COLUMN = 'year'\n",
    "\n",
    "# Feature engineering\n",
    "categorical_columns = ['ENTRYTYPE']\n",
    "\n",
    "featurizer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"title\", TfidfVectorizer(), \"title\"),\n",
    "        (\"author\", TfidfVectorizer(), \"author\"),\n",
    "        (\"ENTRYTYPE\", OneHotEncoder(), categorical_columns),\n",
    "        (\"publisher\", TfidfVectorizer(), \"publisher\")],\n",
    "        remainder='drop')\n",
    "\n",
    "dummy = make_pipeline(featurizer, DummyRegressor(strategy='mean'))\n",
    "ridge = make_pipeline(featurizer, Ridge())\n",
    "\n",
    "logging.info(\"Fitting models\")\n",
    "dummy.fit(train.drop('year', axis=1), train['year'].values)\n",
    "ridge.fit(train.drop('year', axis=1), train['year'].values)\n",
    "\n",
    "# Create pipelines\n",
    "random_forest = make_pipeline(featurizer, RandomForestRegressor(n_estimators = 500, \n",
    "                                                                random_state = 123, n_jobs = -1, min_samples_split = 0.1, max_features = 'auto', max_depth = 18))\n",
    "\n",
    "# Assuming you have your features (X) and target variable (y)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the RandomForestRegressor with some hyperparameters to tune\n",
    "rf_regressor = RandomForestRegressor()\n",
    "\n",
    "# Define the hyperparameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to search for the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=rf_regressor, param_grid=param_grid,\n",
    "                           scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Print the best cross-validated score\n",
    "best_score = grid_search.best_score_\n",
    "print(\"Best Cross-Validated Score (negative mean squared error):\", best_score)\n",
    "\n",
    "# Access the complete results, including all hyperparameter combinations\n",
    "cv_results = grid_search.cv_results_\n",
    "print(\"Complete Cross-Validation Results:\", cv_results)\n",
    "                              \n",
    "                              \n",
    "\n",
    "# Assuming your data is split into features and target\n",
    "X = train.drop(TARGET_COLUMN, axis=1)\n",
    "y = train[TARGET_COLUMN] \n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Fit the model\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = random_forest.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "err = mean_absolute_error(y_test, predictions)\n",
    "print(f'Mean Absolute Error on Test Set: {err}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error on Test Set: 6.594545454545446\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Constants\n",
    "TITLE_COLUMN = 'title'\n",
    "TARGET_COLUMN = 'year'\n",
    "\n",
    "# Feature engineering\n",
    "categorical_columns = ['ENTRYTYPE']\n",
    "\n",
    "featurizer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"title\", TfidfVectorizer(), \"title\"),\n",
    "        (\"author\", TfidfVectorizer(), \"author\"),\n",
    "        (\"ENTRYTYPE\", OneHotEncoder(), categorical_columns),\n",
    "        (\"publisher\", TfidfVectorizer(), \"publisher\")],\n",
    "        remainder='drop')\n",
    "\n",
    "dummy = make_pipeline(featurizer, DummyRegressor(strategy='mean'))\n",
    "ridge = make_pipeline(featurizer, Ridge())\n",
    "\n",
    "logging.info(\"Fitting models\")\n",
    "dummy.fit(train.drop('year', axis=1), train['year'].values)\n",
    "ridge.fit(train.drop('year', axis=1), train['year'].values)\n",
    "\n",
    "# Create pipelines\n",
    "random_forest = make_pipeline(featurizer, RandomForestRegressor(random_state=42))\n",
    "\n",
    "# Assuming your data is split into features and target\n",
    "X = validation_sample.drop(TARGET_COLUMN, axis=1)\n",
    "y = validation_sample[TARGET_COLUMN]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = random_forest.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "err = mean_absolute_error(y_test, predictions)\n",
    "print(f'Mean Absolute Error on Test Set: {err}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import logging\n",
    "\n",
    "# Constants\n",
    "TITLE_COLUMN = 'title'\n",
    "TARGET_COLUMN = 'year'\n",
    "\n",
    "# Feature engineering\n",
    "categorical_columns = ['ENTRYTYPE']\n",
    "\n",
    "featurizer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"title\", TfidfVectorizer(), \"title\"),\n",
    "        (\"author\", TfidfVectorizer(), \"author\"),\n",
    "        (\"ENTRYTYPE\", OneHotEncoder(), categorical_columns),\n",
    "        (\"publisher\", TfidfVectorizer(), \"publisher\")],\n",
    "        remainder='drop')\n",
    "\n",
    "# Create pipelines\n",
    "dummy = make_pipeline(featurizer, DummyRegressor(strategy='mean'))\n",
    "random_forest = make_pipeline(featurizer, RandomForestRegressor())\n",
    "\n",
    "# Define the parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'randomforestregressor__n_estimators': [50, 100, 200],\n",
    "    'randomforestregressor__max_depth': [None, 10, 20],\n",
    "    'randomforestregressor__min_samples_split': [2, 5, 10],\n",
    "    'randomforestregressor__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "print('check')\n",
    "\n",
    "# Perform Grid Search Cross-Validation\n",
    "grid_search = GridSearchCV(random_forest, param_grid, scoring='neg_mean_absolute_error', cv=5)\n",
    "grid_search.fit(train.drop(TARGET_COLUMN, axis=1), train[TARGET_COLUMN].values)\n",
    "\n",
    "# Get the best model and its parameters\n",
    "best_random_forest_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Log the best parameters\n",
    "logging.info(f'Best Parameters: {best_params}')\n",
    "\n",
    "# Evaluate the best model\n",
    "err = mean_absolute_error(validation_sample[TARGET_COLUMN].values, best_random_forest_model.predict(validation_sample.drop(TARGET_COLUMN, axis=1)))\n",
    "logging.info(f'Mean Absolute Error on Validation Set: {err}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validation_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-982e41574820>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# Evaluate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_sample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTARGET_COLUMN\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_forest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_sample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTARGET_COLUMN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Mean Absolute Error on Validation Set: {err}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'validation_sample' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Constants\n",
    "TITLE_COLUMN = 'title'\n",
    "TARGET_COLUMN = 'year'\n",
    "\n",
    "# Feature engineering\n",
    "featurizer = ColumnTransformer(\n",
    "    transformers=[(TITLE_COLUMN, CountVectorizer(), TITLE_COLUMN)],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Create pipelines\n",
    "dummy = make_pipeline(featurizer, DummyRegressor(strategy='mean'))\n",
    "\n",
    "# Simplified Random Forest\n",
    "random_forest = make_pipeline(featurizer, RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1\n",
    "))\n",
    "\n",
    "# Fit the model\n",
    "random_forest.fit(train.drop(TARGET_COLUMN, axis=1), train[TARGET_COLUMN].values)\n",
    "\n",
    "# Evaluate the model\n",
    "err = mean_absolute_error(validation_sample[TARGET_COLUMN].values, random_forest.predict(validation_sample.drop(TARGET_COLUMN, axis=1)))\n",
    "print(f'Mean Absolute Error on Validation Set: {err}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique publishers\n",
    "unique_publishers = train['publisher'].nunique()\n",
    "\n",
    "print(f\"Number of unique publishers: {unique_publishers}\")\n",
    "\n",
    "#number of unique entrytype's\n",
    "unique_entrytype = train['ENTRYTYPE'].nunique()\n",
    "\n",
    "print(f\"Number of unique entrytype's: {unique_entrytype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see the number of publishers and the count of them\n",
    "average_year_per_publisher = train.groupby('publisher')['year'].mean()\n",
    "count_per_publisher = train.groupby('publisher')['year'].count()\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "result_df = pd.DataFrame({\n",
    "    'average_year': average_year_per_publisher,\n",
    "    'count': count_per_publisher\n",
    "}).reset_index()\n",
    "\n",
    "result_df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # Seaborn enhances the appearance of Matplotlib plots\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'train' is your DataFrame and 'column_name' is the column you want to check for outliers\n",
    "column_name = 'year'\n",
    "\n",
    "# Create a boxplot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=train[column_name])\n",
    "\n",
    "# Display the plot\n",
    "plt.title(f'Boxplot for {year}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'train' is your DataFrame and 'column_name' is the column you want to check for outliers\n",
    "column_name = 'year'\n",
    "\n",
    "# Calculate Q1, Q3, and IQR\n",
    "Q1 = train[column_name].quantile(0.25)\n",
    "Q3 = train[column_name].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the lower and upper bounds for outliers\n",
    "lower_bound = Q1 - 3 * IQR\n",
    "upper_bound = Q3 + 3 * IQR\n",
    "\n",
    "\n",
    "# Detect outliers\n",
    "outliers = train[(train[column_name] < lower_bound) | (train[column_name] > upper_bound)]\n",
    "\n",
    "# Display the outliers\n",
    "print(f\"Outliers in {column_name}:\\n{outliers}\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(train[column_name].describe())\n",
    "\n",
    "print(lower_bound)\n",
    "print(upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove outliers below iqr\n",
    "column_name = 'year'\n",
    "\n",
    "# Calculate Q1, Q3, and IQR for the 'year' column\n",
    "Q1 = train[column_name].quantile(0.25)\n",
    "Q3 = train[column_name].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the lower and upper bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "\n",
    "# Remove rows where 'year' is lower than the lower bound\n",
    "train_filtered = train[train[column_name] >= lower_bound]\n",
    "\n",
    "# Display information about removed rows\n",
    "removed_rows = train.shape[0] - train_filtered.shape[0]\n",
    "print(f\"Number of rows removed: {removed_rows}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove outliers below iqr\n",
    "column_name = 'year'\n",
    "\n",
    "# Calculate Q1, Q3, and IQR for the 'year' column\n",
    "Q1 = validation[column_name].quantile(0.25)\n",
    "Q3 = validation[column_name].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the lower and upper bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "\n",
    "# Remove rows where 'year' is lower than the lower bound\n",
    "validation_filtered = validation[validation[column_name] >= lower_bound]\n",
    "\n",
    "# Display information about removed rows\n",
    "removed_rows = validation.shape[0] - validation_filtered.shape[0]\n",
    "print(f\"Number of rows removed: {removed_rows}\")\n",
    "\n",
    "\n",
    "# Assuming only 'ENTRYTYPE' is a categorical column\n",
    "categorical_columns = ['ENTRYTYPE']\n",
    "\n",
    "# Create a column transformer with one-hot encoding for 'ENTRYTYPE' only\n",
    "featurizer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"title\", TfidfVectorizer(), \"title\"),\n",
    "        (\"author\", TfidfVectorizer(), \"author\"),\n",
    "        (\"ENTRYTYPE\", OneHotEncoder(), categorical_columns),\n",
    "        (\"publisher\", CountVectorizer(), \"publisher\")],\n",
    "        remainder='drop')\n",
    "\n",
    "dummy = make_pipeline(featurizer, DummyRegressor(strategy='mean'))\n",
    "ridge = make_pipeline(featurizer, Ridge())\n",
    "\n",
    "logging.info(\"Fitting models\")\n",
    "dummy.fit(train.drop('year', axis=1), train['year'].values)\n",
    "ridge.fit(train.drop('year', axis=1), train['year'].values)\n",
    "\n",
    "err = mean_absolute_error(validation['year'].values, ridge.predict(validation.drop('year', axis=1)))\n",
    "print(validation.dtypes)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "featurizer = ColumnTransformer(\n",
    "        transformers=[(\"title\", CountVectorizer(), \"title\"),\n",
    "        (\"ENTRYTPE\", CountVectorizer(), \"ENTRYTYPE\")]\n",
    "        remainder='drop')\n",
    "Random_Forest = make_pipeline(featurizer, RandomForestRegressor())\n",
    "logging.info(\"Fitting models\")\n",
    "Random_Forest.fit(train.drop('year', axis=1), train['year'].values)\n",
    "err = mean_absolute_error(validation['year'].values, Random_Forest.predict(validation.drop('year', axis=1)))\n",
    "logging.info(f\"Mean baseline MAE: {err}\")\n",
    "print(err)\n",
    "pred = Random_Forest.predict(test)\n",
    "test['year'] = pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do not run\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import logging\n",
    "\n",
    "# Constants\n",
    "TITLE_COLUMN = 'title'\n",
    "TARGET_COLUMN = 'year'\n",
    "\n",
    "# Feature engineering\n",
    "featurizer = ColumnTransformer(\n",
    "    transformers=[(TITLE_COLUMN, CountVectorizer(), TITLE_COLUMN)],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Create pipelines\n",
    "dummy = make_pipeline(featurizer, DummyRegressor(strategy='mean'))\n",
    "random_forest = make_pipeline(featurizer, RandomForestRegressor())\n",
    "\n",
    "# Define the parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'randomforestregressor__n_estimators': [50, 100, 200],\n",
    "    'randomforestregressor__max_depth': [None, 10, 20],\n",
    "    'randomforestregressor__min_samples_split': [2, 5, 10],\n",
    "    'randomforestregressor__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "print('check')\n",
    "\n",
    "# Perform Grid Search Cross-Validation\n",
    "grid_search = GridSearchCV(random_forest, param_grid, scoring='neg_mean_absolute_error', cv=5)\n",
    "grid_search.fit(train.drop(TARGET_COLUMN, axis=1), train[TARGET_COLUMN].values)\n",
    "\n",
    "# Get the best model and its parameters\n",
    "best_random_forest_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Log the best parameters\n",
    "logging.info(f'Best Parameters: {best_params}')\n",
    "\n",
    "# Evaluate the best model\n",
    "err = mean_absolute_error(validation[TARGET_COLUMN].values, best_random_forest_model.predict(validation.drop(TARGET_COLUMN, axis=1)))\n",
    "logging.info(f'Mean Absolute Error on Validation Set: {err}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.dummy import DummyRegressor\n",
    "import logging\n",
    "\n",
    "featurizer = ColumnTransformer(\n",
    "        transformers=[(\"title\", CountVectorizer(), \"title\"),\n",
    "                (\"author\", CountVectorizer(), \"author\"),\n",
    "                (\"ENTRYTYPE\", CountVectorizer(), \"ENTRYTYPE\"),\n",
    "                (\"publisher\", CountVectorizer(), \"publisher\")],\n",
    "        remainder='drop')\n",
    "dummy = make_pipeline(featurizer, DummyRegressor(strategy='mean'))\n",
    "Random_Forest = make_pipeline(featurizer, RandomForestRegressor())\n",
    "logging.info(\"Fitting models\")\n",
    "dummy.fit(train.drop('year', axis=1), train['year'].values)\n",
    "ridge.fit(train.drop('year', axis=1), train['year'].values)\n",
    "err = mean_absolute_error(validation['year'].values, dummy.predict(validation.drop('year', axis=1)))\n",
    "print(err)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info(\"Loading training/test data\")\n",
    "    train = pd.DataFrame.from_records(json.load(open('train.json'))).fillna(\"\")\n",
    "    test = pd.DataFrame.from_records(json.load(open('test.json'))).fillna(\"\")\n",
    "    logging.info(\"Splitting validation\")\n",
    "    train, val = train_test_split(train, stratify=train['year'], random_state=123)\n",
    "    featurizer = ColumnTransformer(\n",
    "        transformers=[(\"title\", CountVectorizer(), \"title\")],\n",
    "        remainder='drop')\n",
    "    dummy = make_pipeline(featurizer, DummyRegressor(strategy='mean'))\n",
    "    ridge = make_pipeline(featurizer, Ridge())\n",
    "    logging.info(\"Fitting models\")\n",
    "    dummy.fit(train.drop('year', axis=1), train['year'].values)\n",
    "    ridge.fit(train.drop('year', axis=1), train['year'].values)\n",
    "    logging.info(\"Evaluating on validation data\")\n",
    "    err = mean_absolute_error(val['year'].values, dummy.predict(val.drop('year', axis=1)))\n",
    "    logging.info(f\"Mean baseline MAE: {err}\")\n",
    "    err = mean_absolute_error(val['year'].values, ridge.predict(val.drop('year', axis=1)))\n",
    "    logging.info(f\"Ridge regress MAE: {err}\")\n",
    "    logging.info(f\"Predicting on test\")\n",
    "    pred = ridge.predict(test)\n",
    "    test['year'] = pred\n",
    "    logging.info(\"Writing prediction file\")\n",
    "    test.to_json(\"predicted.json\", orient='records', indent=2)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0cc613358bbbc4846d9e92ed7a7dca7b66c2d79abdfaf761fbc67259619149a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
